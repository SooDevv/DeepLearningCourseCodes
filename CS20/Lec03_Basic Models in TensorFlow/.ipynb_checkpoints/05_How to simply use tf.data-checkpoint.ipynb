{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to simply use tf.data\n",
    "> tf.data package를 사용하는 방법에 관한 예시, 예시 데이터는 numpy package를 이용하여 간단하게 data에 해당하는 X, target에 해당하는 y를 생성하여 tf.data package의 각종 module, function을 이용한다. epoch 마다 validation data에 대해서 validation을 하는 상황을 가정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Template\n",
    "> for문을 활용, model을 training시 data pipeline으로써 아래의 function과 method를 사용하는 방법에 대한 예시\n",
    "\n",
    "- Dataset class \n",
    "    + tf.data.Dataset.from_tensor_slices으로 Dataset class의 instance를 생성\n",
    "        * train data에 대한 Dataset class의 instance, tr_data\n",
    "        * validation data에 대한 Dataset class의 instance, val_data\n",
    "    + 아래와 같은 method를 활용하여 training 시 필요한 요소를 지정 \n",
    "        * instance의 shuffle method를 활용하여 shuffling\n",
    "        * instance의 batch method를 활용하여 batch size 지정 \n",
    "        * for문으로 전체 epoch를 control하므로 repeat method는 활용하지 않음 \n",
    "        \n",
    "<br>\n",
    "- Iterator class\n",
    "    + Dataset class의 instance에서 make_initializable_iterator method로 iterator class의 instance를 생성 \n",
    "        * train data에 대한 iterator class의 instance, tr_iterator\n",
    "        * validation data에 대한 iterator class의 instance. val_iterator\n",
    "        * make_initializable_iterator()로 iterator class의 instance를 생성할 경우, random_seed를 고정x \n",
    "            - random_seed를 고정할 경우, 서로 다른 epoch의 step별 mini-batch의 구성이 완전히 똑같아지기 때문\n",
    "    + Anonymous iterator를 tf.data.iterator.from_string_handle로 생성\n",
    "        * string_handle argument에 tf.placeholder를 이용\n",
    "            - tr_iterator를 활용할 것인지, val_iterator를 활용할 것인지 조절 \n",
    "            \n",
    "            <br>\n",
    "- Reference\n",
    "  + [aisolab, 'How to simply use tf.data'](https://github.com/aisolab/CS20/blob/master/Lec03_Linear%20and%20Logistic%20Regression/How%20to%20simply%20use%20tf.data.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## when use tf.data?\n",
    "- For prototype, feed dict can be faster and easier to write\n",
    "- tf.data is tricky to use when you have complicated preprocessing or multiple data sources\n",
    "- NLP data is normally just a sequence of integers. In this case, transferring the data over to GPU is pretty quick, so the speedup of tf.data isn;t that large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 2) (12,)\n",
      "[[ 0  0]\n",
      " [ 1  1]\n",
      " [ 2  2]\n",
      " [ 3  3]\n",
      " [ 4  4]\n",
      " [ 5  5]\n",
      " [ 6  6]\n",
      " [ 7  7]\n",
      " [ 8  8]\n",
      " [ 9  9]\n",
      " [10 10]\n",
      " [11 11]]\n"
     ]
    }
   ],
   "source": [
    "# 전체 데이터의 개수가 12개인 임의의 데이터셋 생성\n",
    "x = np.c_[np.arange(12), np.arange(12)]\n",
    "y = np.arange(12)\n",
    "\n",
    "print(x.shape, y.shape)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 2) (8,)\n",
      "(4, 2) (4,)\n"
     ]
    }
   ],
   "source": [
    "# 위의 데이터를 train, validation으로 split\n",
    "x_tr = x[:8]\n",
    "y_tr = y[:8]\n",
    "\n",
    "x_val = x[8:]\n",
    "y_val = y[8:]\n",
    "\n",
    "print(x_tr.shape, y_tr.shape)\n",
    "print(x_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3, batch_size: 2, total_steps: 6\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 3\n",
    "batch_size = 2\n",
    "total_steps = int(x.shape[0] / batch_size)\n",
    "\n",
    "print('epoch: {}, batch_size: {}, total_steps: {}'.format(n_epoch, batch_size, total_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((?, 2), (?,)), types: (tf.int32, tf.int32)>\n",
      "<BatchDataset shapes: ((?, 2), (?,)), types: (tf.int32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "tr_data = tf.data.Dataset.from_tensor_slices((x_tr, y_tr))\n",
    "tr_data = tr_data.shuffle(buffer_size=30)\n",
    "tr_data = tr_data.batch(batch_size=batch_size)\n",
    "\n",
    "val_data = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_data = val_data.batch(batch_size=batch_size)\n",
    "\n",
    "print(tr_data)\n",
    "print(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_iterator = tr_data.make_initializable_iterator()\n",
    "val_iterator = val_data.make_initializable_iterator()\n",
    "\n",
    "handle = tf.placeholder(dtype=tf.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = tf.data.Iterator.from_string_handle(string_handle=handle,\n",
    "                                              output_shapes=tr_iterator.output_shapes,\n",
    "                                              output_types=tr_iterator.output_types)\n",
    "x_mb, y_mb = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 training start\n",
      "step: 1\n",
      "[[10 10]\n",
      " [11 11]] [5 7]\n",
      "step: 2\n",
      "[[10 10]\n",
      " [11 11]] [0 1]\n",
      "step: 3\n",
      "[[10 10]\n",
      " [11 11]] [3 6]\n",
      "step: 4\n",
      "[[10 10]\n",
      " [11 11]] [2 4]\n",
      "epoch: 1 training finished\n",
      "at epoch: 1, validation start\n",
      "step: 1\n",
      "[[8 8]\n",
      " [9 9]] [8 9]\n",
      "step: 2\n",
      "[[10 10]\n",
      " [11 11]] [10 11]\n",
      "validation finished\n",
      "epoch: 2 training start\n",
      "step: 1\n",
      "[[10 10]\n",
      " [11 11]] [4 2]\n",
      "step: 2\n",
      "[[10 10]\n",
      " [11 11]] [1 7]\n",
      "step: 3\n",
      "[[10 10]\n",
      " [11 11]] [6 0]\n",
      "step: 4\n",
      "[[10 10]\n",
      " [11 11]] [3 5]\n",
      "epoch: 2 training finished\n",
      "at epoch: 2, validation start\n",
      "step: 1\n",
      "[[8 8]\n",
      " [9 9]] [8 9]\n",
      "step: 2\n",
      "[[10 10]\n",
      " [11 11]] [10 11]\n",
      "validation finished\n",
      "epoch: 3 training start\n",
      "step: 1\n",
      "[[10 10]\n",
      " [11 11]] [6 1]\n",
      "step: 2\n",
      "[[10 10]\n",
      " [11 11]] [4 0]\n",
      "step: 3\n",
      "[[10 10]\n",
      " [11 11]] [5 7]\n",
      "step: 4\n",
      "[[10 10]\n",
      " [11 11]] [3 2]\n",
      "epoch: 3 training finished\n",
      "at epoch: 3, validation start\n",
      "step: 1\n",
      "[[8 8]\n",
      " [9 9]] [8 9]\n",
      "step: 2\n",
      "[[10 10]\n",
      " [11 11]] [10 11]\n",
      "validation finished\n"
     ]
    }
   ],
   "source": [
    "# n_tr_step, n_val_step 변수와 관련된 코드는 step수 확인을 위해 넣음\n",
    "sess = tf.Session()\n",
    "tr_handle, val_handle = sess.run([tr_iterator.string_handle(), val_iterator.string_handle()])\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    print('epoch: {} training start'.format(epoch+1))\n",
    "    sess.run(tr_iterator.initializer) # Run tr_iterator\n",
    "    n_tr_step = 0\n",
    "    while True:\n",
    "        try:\n",
    "            n_tr_step += 1\n",
    "            X_tmp, y_tmp = sess.run([x_mb, y_mb], feed_dict = {handle : tr_handle})\n",
    "            print('step: {}'.format(n_tr_step))\n",
    "            print(x_tmp, y_tmp)\n",
    "        except:\n",
    "            print('epoch: {} training finished'.format(epoch+1))\n",
    "            break\n",
    "    \n",
    "    print('at epoch: {}, validation start'.format(epoch+1))\n",
    "    sess.run(val_iterator.initializer)\n",
    "    n_val_step = 0\n",
    "    while True:\n",
    "        try:\n",
    "            n_val_step += 1\n",
    "            x_tmp, y_tmp = sess.run([x_mb, y_mb], feed_dict={handle: val_handle})\n",
    "            \n",
    "            print('step: {}'.format(n_val_step))\n",
    "            print(x_tmp, y_tmp)\n",
    "        except:\n",
    "            print('validation finished')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
